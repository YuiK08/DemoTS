import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import timedelta
import tensorflow as tf
import os
import logging
import joblib
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import load_model
# === Logging ===
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# === Streamlit UI settings ===
st.set_page_config(page_title="D·ª± b√°o Nhu c·∫ßu ƒêi·ªán", layout="wide")
st.markdown("""
    <style>
    .main { background-color: #F9FAFC; }
    .sidebar .sidebar-content { background-color: #F0F2F6; padding: 10px }
    .block-container { padding-top: 1rem; padding-bottom: 1rem; }
    .stSelectbox, .stDateInput { font-size: 16px; }
    .stWarning { color: #FFA500; }
    .stError { color: #FF0000; }
    </style>
""", unsafe_allow_html=True)

# === Title ===
st.title("üîå D·ª± b√°o Si√™u Chi Ti·∫øt Nhu c·∫ßu ƒêi·ªán NƒÉng")
st.markdown("64TTNT2")

# === Sidebar: ch·ªçn model & th·ªùi gian ===
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")

    model_options = {
        "LSTM": "lstm_full_model.h5",
        "GRU": "gru_full_model.h5",
        "Informer": "informer_full_model"  # ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a .pb
    }
    selected_model_name = st.selectbox("üîç Ch·ªçn m√¥ h√¨nh", list(model_options.keys()))
    model_path = model_options[selected_model_name]

    file_path = "datathugon.csv"
    default_start = pd.to_datetime("2020-01-01")
    default_end = pd.to_datetime("2020-12-31")
    date_range = st.date_input("üìÖ Ch·ªçn kho·∫£ng th·ªùi gian d·ª± b√°o", [default_start, default_end])
    start_date, end_date = pd.to_datetime(date_range[0]), pd.to_datetime(date_range[1])

# === Load d·ªØ li·ªáu t·ª´ CSV theo chunk ===
@st.cache_data(show_spinner="üîÑ ƒêang t·∫£i d·ªØ li·ªáu...")
def load_data_chunked(file_path, forecast_start, forecast_end):
    dtypes = {
        "Electricity_Consumed": "float32",
        "Temperature": "float32",
        "Humidity": "float32",
        "Wind_Speed": "float32",
        "Avg_Past_Consumption": "float32"
    }
    parse_dates = ["Timestamp"]
    usecols = list(dtypes.keys()) + ["Timestamp"]
    historical_start = pd.to_datetime("2000-01-01")
    historical_end = pd.to_datetime("2025-01-01")
    chunks = []

    try:
        logger.info(f"ƒêang ƒë·ªçc file: {file_path}")
        if not os.path.exists(file_path):
            st.error(f"T·ªáp d·ªØ li·ªáu {file_path} kh√¥ng t·ªìn t·∫°i!")
            st.stop()
        reader = pd.read_csv(file_path, dtype=dtypes, parse_dates=parse_dates,
                             usecols=usecols, chunksize=100_000,
                             on_bad_lines='skip', low_memory=False)
        for chunk in reader:
            mask = (chunk["Timestamp"] >= historical_start - timedelta(days=2)) & \
                   (chunk["Timestamp"] <= historical_end)
            chunk = chunk[mask]
            if not chunk.empty:
                chunk.set_index("Timestamp", inplace=True)
                numeric_cols = chunk.select_dtypes(include=['number']).columns
                chunk = chunk[numeric_cols].resample("30min").mean().dropna()
                chunks.append(chunk)
        if chunks:
            df = pd.concat(chunks).groupby(level=0).mean().asfreq("30min").dropna()
            logger.info(f"ƒê√£ t·∫£i th√†nh c√¥ng d·ªØ li·ªáu v·ªõi {len(df)} d√≤ng.")
            return df
        else:
            logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng th·ªùi gian 2000-2018.")
            return pd.DataFrame()
    except Exception as e:
        logger.error(f"L·ªói khi t·∫£i d·ªØ li·ªáu: {e}")
        st.error(f"‚ö†Ô∏è L·ªói t·∫£i d·ªØ li·ªáu: {e}")
        return pd.DataFrame()

df = load_data_chunked(file_path, start_date, end_date)

if df.empty:
    st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c ch·ªçn.")
    st.stop()

latest_year = df.index.max().year
if start_date.year > latest_year:
    st.warning(f"‚ö†Ô∏è D·ªØ li·ªáu ch·ªâ ƒë·∫øn nƒÉm {latest_year}. ƒêi·ªÅu ch·ªânh d·ª± b√°o.")
    start_date = pd.to_datetime(f"{latest_year}-01-01")
    end_date = pd.to_datetime(f"{latest_year}-12-31")

# === Ti·ªÅn x·ª≠ l√Ω ƒë·∫∑c tr∆∞ng ===
df["hour"] = df.index.hour
df["dayofweek"] = df.index.dayofweek
df["dayofyear"] = df.index.dayofyear
df["is_weekend"] = df["dayofweek"].isin([5, 6]).astype(int)

df["hour_sin"] = np.sin(2 * np.pi * df["hour"] / 24)
df["hour_cos"] = np.cos(2 * np.pi * df["hour"] / 24)
df["dayofweek_sin"] = np.sin(2 * np.pi * df["dayofweek"] / 7)
df["dayofweek_cos"] = np.cos(2 * np.pi * df["dayofweek"] / 7)
df["dayofyear_sin"] = np.sin(2 * np.pi * df["dayofyear"] / 365.25)
df["dayofyear_cos"] = np.cos(2 * np.pi * df["dayofyear"] / 365.25)

df["lag_1h"] = df["Electricity_Consumed"].shift(2)
df["lag_24h"] = df["Electricity_Consumed"].shift(48)
df["lag_168h"] = df["Electricity_Consumed"].shift(48*7)

df.dropna(inplace=True)

# === ƒê·∫∑c tr∆∞ng ƒë·∫ßu v√†o v√† target ===
features_x = [
    "Electricity_Consumed",
    "lag_1h", "lag_24h", "lag_168h",
    "hour_sin", "hour_cos",
    "dayofweek_sin", "dayofweek_cos",
    "dayofyear_sin", "dayofyear_cos",
    "is_weekend"
]  # 11 features
target = "Electricity_Consumed"

# === Load ho·∫∑c hu·∫•n luy·ªán scaler_y (ch·ªâ gi·ªØ scaler_y) ===
scaler_y_path = f"{selected_model_name.lower()}_scaler_y.pkl"

try:
    scaler_y = joblib.load(scaler_y_path)
    logger.info(f"ƒê√£ t·∫£i scaler_y t·ª´ {scaler_y_path}.")
except FileNotFoundError:
    logger.warning(f"Kh√¥ng t√¨m th·∫•y scaler_y: {scaler_y_path}. ƒêang hu·∫•n luy·ªán l·∫°i...")
    st.warning("Scaler_y kh√¥ng t·ªìn t·∫°i. ƒêang hu·∫•n luy·ªán l·∫°i scaler_y...")
    scaler_y = StandardScaler()
    scaler_y.fit(df[[target]])
    joblib.dump(scaler_y, scaler_y_path)
    logger.info(f"ƒê√£ hu·∫•n luy·ªán v√† l∆∞u scaler_y.")

# === Load m√¥ h√¨nh ===
if selected_model_name in ["LSTM", "GRU"]:
    model = load_model(model_path)
    logger.info(f"ƒê√£ t·∫£i m√¥ h√¨nh {selected_model_name} t·ª´ {model_path}.")
elif selected_model_name == "Informer":
    try:
        # ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c SavedModel
        model = tf.saved_model.load(model_path)
        logger.info(f"ƒê√£ t·∫£i m√¥ h√¨nh Informer t·ª´ {model_path}.")
    except Exception as e:
        logger.error(f"L·ªói khi t·∫£i m√¥ h√¨nh Informer: {e}")
        st.error(f"‚ö†Ô∏è L·ªói t·∫£i m√¥ h√¨nh Informer: {e}")
        st.stop()

# === D·ª± b√°o ===
try:
    # Chu·∫©n b·ªã d·ªØ li·ªáu (d√πng d·ªØ li·ªáu th√¥ ƒë√£ chu·∫©n h√≥a)
    df_x = df[features_x].copy()
    df_y = df[[target]].copy()
    X_raw = df_x.values  # D·ªØ li·ªáu th√¥ ƒë√£ chu·∫©n h√≥a
    y_scaled = scaler_y.transform(df_y.values)  # Chu·∫©n h√≥a target ban ƒë·∫ßu

    seq_length = 48
    data_seq_x = np.array([X_raw[i:i+seq_length] for i in range(len(X_raw) - seq_length)])
    data_seq_y = np.array([y_scaled[i:i+seq_length] for i in range(len(y_scaled) - seq_length)])

    logger.info(f"Shape of data_seq_x: {data_seq_x.shape}")
    logger.info(f"Shape of data_seq_y: {data_seq_y.shape}")

    # T√≠nh time_idx tr∆∞·ªõc khi d·ª± ƒëo√°n
    time_idx = df_x.index[seq_length:seq_length + len(data_seq_x)]

    if selected_model_name in ["LSTM", "GRU"]:
        n_inputs = len(model.inputs)
        logger.info(f"S·ªë ƒë·∫ßu v√†o mong ƒë·ª£i b·ªüi m√¥ h√¨nh: {n_inputs}")

        if n_inputs == 2:  # Encoder-Decoder (GRU)
            decoder_input = np.zeros((data_seq_x.shape[0], 24, 1), dtype=np.float32)
            y_pred_scaled = model.predict([data_seq_x, decoder_input], batch_size=32)
        elif n_inputs == 1:  # Ch·ªâ encoder (LSTM)
            y_pred_scaled = model.predict(data_seq_x, batch_size=32)
        else:
            raise ValueError(f"M√¥ h√¨nh y√™u c·∫ßu {n_inputs} ƒë·∫ßu v√†o, kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
    elif selected_model_name == "Informer":
        # Chuy·ªÉn d·ªØ li·ªáu sang ƒë·ªãnh d·∫°ng TensorFlow
        data_seq_x_tensor = tf.convert_to_tensor(data_seq_x, dtype=tf.float32)
        # G·ªçi m√¥ h√¨nh SavedModel (c·∫ßn ki·ªÉm tra signature_def)
        try:
            # Ki·ªÉm tra signature c√≥ s·∫µn
            signatures = list(model.signatures.keys())
            logger.info(f"Signatures available: {signatures}")
            if "serving_default" in signatures:
                y_pred_scaled = model.signatures["serving_default"](data_seq_x_tensor)
                y_pred_scaled = y_pred_scaled['output_0']  # ƒêi·ªÅu ch·ªânh t√™n output
            else:
                raise ValueError("Kh√¥ng t√¨m th·∫•y signature 'serving_default'")
        except Exception as e:
            logger.error(f"L·ªói khi g·ªçi m√¥ h√¨nh Informer: {e}")
            st.error(f"‚ö†Ô∏è L·ªói g·ªçi m√¥ h√¨nh Informer: {e}")
            st.stop()
        y_pred_scaled = y_pred_scaled.numpy()  # Chuy·ªÉn v·ªÅ numpy array

    logger.info(f"Shape of y_pred_scaled: {y_pred_scaled.shape}")

    # Chuy·ªÉn ng∆∞·ª£c v·ªÅ gi√° tr·ªã g·ªëc (ch·ªâ √°p d·ª•ng cho ƒë·∫ßu ra 2D)
    if y_pred_scaled.ndim == 3:
        y_pred_scaled_2d = y_pred_scaled.reshape(-1, y_pred_scaled.shape[-1])
    else:
        y_pred_scaled_2d = y_pred_scaled
    y_pred = scaler_y.inverse_transform(y_pred_scaled_2d)
    y_pred = y_pred.flatten()[:len(time_idx)]  # C·∫Øt v·ªÅ ƒë√∫ng s·ªë m·∫´u

    df_result = pd.DataFrame({"Timestamp": time_idx, "D·ª± b√°o": y_pred}).set_index("Timestamp")
    df_result = df_result[(df_result.index >= start_date) & (df_result.index <= end_date)]

    logger.info(f"‚úÖ D·ª± b√°o th√†nh c√¥ng v·ªõi {len(df_result)} d√≤ng.")
except Exception as e:
    logger.error(f"L·ªói d·ª± b√°o: {e}")
    st.error(f"‚ö†Ô∏è L·ªói d·ª± b√°o: {e}")
    st.stop()

# === Hi·ªÉn th·ªã k·∫øt qu·∫£ ===
st.success(f"‚úÖ ƒê√£ d·ª± b√°o th√†nh c√¥ng v·ªõi m√¥ h√¨nh {selected_model_name}")

col1, col2 = st.columns([2, 1])
with col1:
    st.subheader("üìà Bi·ªÉu ƒë·ªì D·ª± b√°o")
    df_historical = df[[target]].rename(columns={target: "D·ªØ li·ªáu g·ªëc"})
    df_combined = pd.concat([df_historical, df_result])
    fig, ax = plt.subplots(figsize=(12, 5))
    df_combined.plot(ax=ax, color={"D·ªØ li·ªáu g·ªëc": "green", "D·ª± b√°o": "royalblue"}, label=["D·ªØ li·ªáu g·ªëc", "D·ª± b√°o"])
    ax.set_ylabel("Nhu c·∫ßu ƒëi·ªán (kWh)")
    ax.set_xlabel("Th·ªùi gian")
    ax.set_title(f"D·ª± b√°o nhu c·∫ßu ƒëi·ªán ({start_date.year} - {end_date.year})")
    ax.grid(True, linestyle='--', alpha=0.3)
    ax.legend()
    st.pyplot(fig)

with col2:
    st.subheader("üìä Th·ªëng k√™ d·ª± b√°o")
    st.dataframe(df_result.describe().T, use_container_width=True)

    st.subheader("üßæ To√†n b·ªô d·ªØ li·ªáu d·ª± b√°o")
    st.dataframe(df_result, use_container_width=True)

    csv = df_result.reset_index().to_csv(index=False).encode("utf-8")
    st.download_button("üì• T·∫£i d·ªØ li·ªáu d·ª± b√°o", data=csv, file_name="du_bao_nhu_cau_dien.csv", mime="text/csv")

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import timedelta
import tensorflow as tf
import logging
import joblib
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import load_model
import pyarrow.parquet as pq
from datasets import load_dataset
from huggingface_hub import hf_hub_download

# === C·∫•u h√¨nh Logging ===
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# === ƒê·∫£m b·∫£o c·ªïng cho Hugging Face Spaces ===
import os
os.environ["PORT"] = "7860"  # C·ªïng m·∫∑c ƒë·ªãnh c·ªßa Hugging Face Spaces

# === C·∫•u h√¨nh giao di·ªán Streamlit ===
st.set_page_config(page_title="D·ª± b√°o Nhu c·∫ßu ƒêi·ªán", layout="wide")
st.markdown("""
    <style>
    .main { background-color: #F9FAFC; }
    .sidebar .sidebar-content { background-color: #F0F2F6; padding: 10px }
    .block-container { padding-top: 1rem; padding-bottom: 1rem; }
    .stSelectbox, .stDateInput { font-size: 16px; }
    .stWarning { color: #FFA500; }
    .stError { color: #FF0000; }
    </style>
""", unsafe_allow_html=True)

# === Ti√™u ƒë·ªÅ ===
st.title("üîå D·ª± b√°o Si√™u Chi Ti·∫øt Nhu c·∫ßu ƒêi·ªán NƒÉng")
st.markdown("64TTNT2")

# === Sidebar: Ch·ªçn m√¥ h√¨nh & th·ªùi gian ===
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    model_options = {
        "LSTM": "lstm_full_model.h5",
        "GRU": "gru_full_model.h5",
        "Informer": "informer_full_model"
    }
    selected_model_name = st.selectbox("üîç Ch·ªçn m√¥ h√¨nh", list(model_options.keys()))
    model_path_key = model_options[selected_model_name]
    dataset_name = "Yu08/DemoTS"  # Thay b·∫±ng t√™n dataset c·ªßa b·∫°n
    parquet_path = "data/data11gb.parquet"  # ƒê∆∞·ªùng d·∫´n trong dataset
    default_start = pd.to_datetime("2020-01-01")
    default_end = pd.to_datetime("2020-12-31")
    date_range = st.date_input("üìÖ Ch·ªçn kho·∫£ng th·ªùi gian d·ª± b√°o", [default_start, default_end])
    start_date, end_date = pd.to_datetime(date_range[0]), pd.to_datetime(date_range[1])

# === Load d·ªØ li·ªáu t·ª´ Hugging Face Datasets ===
@st.cache_data(show_spinner="üîÑ ƒêang t·∫£i d·ªØ li·ªáu (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...", persist=True)
def load_data_parquet(dataset_name, parquet_path, start_date, end_date):
    try:
        start_time = pd.Timestamp.now()
        dataset = load_dataset(dataset_name, data_files=parquet_path, streaming=True)
        df = pd.DataFrame(dataset["train"])
        df["Timestamp"] = pd.to_datetime(df["Timestamp"])
        df = df[(df["Timestamp"] >= start_date - timedelta(days=2)) & 
                (df["Timestamp"] <= end_date)]
        df.set_index("Timestamp", inplace=True)
        numeric_cols = df.select_dtypes(include=['number']).columns
        df = df[numeric_cols].resample("30min").mean().dropna()
        end_time = pd.Timestamp.now()
        logger.info(f"T·ªïng th·ªùi gian ƒë·ªçc v√† x·ª≠ l√Ω: {(end_time - start_time).total_seconds()} gi√¢y")
        if df.empty:
            st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng th·ªùi gian ƒë∆∞·ª£c ch·ªçn.")
            st.stop()
        return df
    except Exception as e:
        logger.error(f"L·ªói khi t·∫£i d·ªØ li·ªáu: {e}")
        st.error(f"‚ö†Ô∏è L·ªói t·∫£i d·ªØ li·ªáu: {e}")
        st.stop()

df = load_data_parquet(dataset_name, parquet_path, start_date, end_date)

# === Ti·ªÅn x·ª≠ l√Ω ƒë·∫∑c tr∆∞ng ===
df["hour"] = df.index.hour
df["dayofweek"] = df.index.dayofweek
df["dayofyear"] = df.index.dayofyear
df["hour_sin"] = np.sin(2 * np.pi * df["hour"] / 24)
df["hour_cos"] = np.cos(2 * np.pi * df["hour"] / 24)
df["dayofweek_sin"] = np.sin(2 * np.pi * df["dayofweek"] / 7)
df["dayofweek_cos"] = np.cos(2 * np.pi * df["dayofweek"] / 7)
df["dayofyear_sin"] = np.sin(2 * np.pi * df["dayofyear"] / 365.25)
df["dayofyear_cos"] = np.cos(2 * np.pi * df["dayofyear"] / 365.25)
df["lag_1h"] = df["Electricity_Consumed"].shift(2)
df["lag_24h"] = df["Electricity_Consumed"].shift(48)
df["lag_168h"] = df["Electricity_Consumed"].shift(48*7)
df.dropna(inplace=True)

# === ƒê·∫∑c tr∆∞ng ƒë·∫ßu v√†o v√† target ===
features_x = [
    "Electricity_Consumed",
    "lag_1h", "lag_24h", "lag_168h",
    "hour_sin", "hour_cos",
    "dayofweek_sin", "dayofweek_cos",
    "dayofyear_sin", "dayofyear_cos"
]
target = "Electricity_Consumed"

# === Load ho·∫∑c hu·∫•n luy·ªán scaler_y ===
scaler_y_path = hf_hub_download(repo_id=dataset_name, 
                                filename=f"models/{selected_model_name.lower()}_scaler_y.pkl", 
                                repo_type="dataset")
try:
    scaler_y = joblib.load(scaler_y_path)
    logger.info(f"ƒê√£ t·∫£i scaler_y t·ª´ {scaler_y_path}.")
except FileNotFoundError:
    logger.warning(f"Kh√¥ng t√¨m th·∫•y scaler_y: {scaler_y_path}. ƒêang hu·∫•n luy·ªán l·∫°i...")
    st.warning("Scaler_y kh√¥ng t·ªìn t·∫°i. ƒêang hu·∫•n luy·ªán l·∫°i scaler_y...")
    scaler_y = StandardScaler()
    scaler_y.fit(df[[target]])
    joblib.dump(scaler_y, scaler_y_path)  # L∆∞u t·∫°m tr√™n Spaces (kh√¥ng ·ªïn ƒë·ªãnh, xem l∆∞u √Ω)
    logger.info(f"ƒê√£ hu·∫•n luy·ªán v√† l∆∞u scaler_y.")

# === Load m√¥ h√¨nh ===
if selected_model_name in ["LSTM", "GRU"]:
    try:
        model_path = hf_hub_download(repo_id=dataset_name, 
                                    filename=f"models/{selected_model_name.lower()}_full_model.h5", 
                                    repo_type="dataset")
        model = load_model(model_path)
        logger.info(f"ƒê√£ t·∫£i m√¥ h√¨nh {selected_model_name} t·ª´ {model_path}.")
    except Exception as e:
        logger.error(f"L·ªói khi t·∫£i m√¥ h√¨nh {selected_model_name}: {e}")
        st.error(f"‚ö†Ô∏è L·ªói t·∫£i m√¥ h√¨nh {selected_model_name}: {e}")
        st.stop()
elif selected_model_name == "Informer":
    try:
        model_path = hf_hub_download(repo_id=dataset_name, 
                                    filename="models/informer_full_model", 
                                    repo_type="dataset")
        model = tf.saved_model.load(model_path)
        logger.info(f"ƒê√£ t·∫£i m√¥ h√¨nh Informer t·ª´ {model_path}.")
    except Exception as e:
        logger.error(f"L·ªói khi t·∫£i m√¥ h√¨nh Informer: {e}")
        st.error(f"‚ö†Ô∏è L·ªói t·∫£i m√¥ h√¨nh Informer: {e}")
        st.stop()

# === H√†m t·∫°o ƒë·∫∑c tr∆∞ng cho d·ª± b√°o t∆∞∆°ng lai ===
def generate_future_features(df, start_date, end_date, seq_length=48):
    future_index = pd.date_range(start=start_date, end=end_date, freq="30min")
    future_df = pd.DataFrame(index=future_index)
    
    future_df["hour"] = future_df.index.hour
    future_df["dayofweek"] = future_df.index.dayofweek
    future_df["dayofyear"] = future_df.index.dayofyear
    future_df["hour_sin"] = np.sin(2 * np.pi * future_df["hour"] / 24)
    future_df["hour_cos"] = np.cos(2 * np.pi * future_df["hour"] / 24)
    future_df["dayofweek_sin"] = np.sin(2 * np.pi * future_df["dayofweek"] / 7)
    future_df["dayofweek_cos"] = np.cos(2 * np.pi * future_df["dayofweek"] / 7)
    future_df["dayofyear_sin"] = np.sin(2 * np.pi * future_df["dayofyear"] / 365.25)
    future_df["dayofyear_cos"] = np.cos(2 * np.pi * df["dayofyear"] / 365.25)
    
    last_year_data = df[df.index.year == df.index.max().year][["Electricity_Consumed"]]
    if len(last_year_data) < seq_length:
        st.error(f"D·ªØ li·ªáu nƒÉm {df.index.max().year} kh√¥ng ƒë·ªß {seq_length} m·∫´u ƒë·ªÉ t·∫°o ƒë·∫∑c tr∆∞ng!")
        st.stop()
    if len(last_year_data) < 48*7:
        logger.warning(f"D·ªØ li·ªáu nƒÉm {df.index.max().year} kh√¥ng ƒë·ªß {48*7} m·∫´u cho lag_168h.")
    
    future_df["Electricity_Consumed"] = np.nan
    future_df["lag_1h"] = np.nan
    future_df["lag_24h"] = np.nan
    future_df["lag_168h"] = np.nan
    
    for i in range(len(future_df)):
        if i < 2 and i-2 < len(last_year_data):
            future_df.iloc[i, future_df.columns.get_loc("lag_1h")] = last_year_data.iloc[-2+i]["Electricity_Consumed"]
        elif i >= 2:
            future_df.iloc[i, future_df.columns.get_loc("lag_1h")] = future_df["Electricity_Consumed"].iloc[i-2] if pd.notna(future_df["Electricity_Consumed"].iloc[i-2]) else last_year_data.iloc[-2+i]["Electricity_Consumed"] if i-2 < len(last_year_data) else np.nan
        
        if i < 48 and i-48 < len(last_year_data):
            future_df.iloc[i, future_df.columns.get_loc("lag_24h")] = last_year_data.iloc[-48+i]["Electricity_Consumed"]
        elif i >= 48:
            future_df.iloc[i, future_df.columns.get_loc("lag_24h")] = future_df["Electricity_Consumed"].iloc[i-48] if pd.notna(future_df["Electricity_Consumed"].iloc[i-48]) else last_year_data.iloc[-48+i]["Electricity_Consumed"] if i-48 < len(last_year_data) else np.nan
        
        if i < 48*7 and i-48*7 < len(last_year_data):
            future_df.iloc[i, future_df.columns.get_loc("lag_168h")] = last_year_data.iloc[-48*7+i]["Electricity_Consumed"] if i-48*7 < len(last_year_data) else np.nan
        elif i >= 48*7:
            future_df.iloc[i, future_df.columns.get_loc("lag_168h")] = future_df["Electricity_Consumed"].iloc[i-48*7] if pd.notna(future_df["Electricity_Consumed"].iloc[i-48*7]) else last_year_data.iloc[-48*7+i]["Electricity_Consumed"] if i-48*7 < len(last_year_data) else np.nan
    
    return future_df[features_x]

# === D·ª± b√°o ===
try:
    seq_length = 48
    latest_year = df.index.max().year
    is_future_forecast = start_date.year > latest_year
    
    if is_future_forecast:
        st.warning(f"‚ö†Ô∏è D·ª± b√°o cho t∆∞∆°ng lai ({start_date.year}). S·ª≠ d·ª•ng d·ªØ li·ªáu nƒÉm {latest_year} l√†m c∆° s·ªü.")
        df_x = generate_future_features(df, start_date, end_date, seq_length)
        time_idx = df_x.index
    else:
        df_x = df[features_x].copy()
        df_y = df[[target]].copy()
        time_idx = df_x.index[seq_length:len(df_x) - seq_length]

    X_raw = df_x.values
    data_seq_x = np.array([X_raw[i:i+seq_length] for i in range(len(X_raw) - seq_length)])

    logger.info(f"Shape of data_seq_x: {data_seq_x.shape}")

    if selected_model_name in ["LSTM", "GRU"]:
        n_inputs = len(model.inputs)
        logger.info(f"S·ªë ƒë·∫ßu v√†o mong ƒë·ª£i b·ªüi m√¥ h√¨nh: {n_inputs}")
        if n_inputs == 2:
            decoder_input = np.zeros((data_seq_x.shape[0], 24, 1), dtype=np.float32)
            y_pred_scaled = model.predict([data_seq_x, decoder_input], batch_size=32)
        elif n_inputs == 1:
            y_pred_scaled = model.predict(data_seq_x, batch_size=32)
        else:
            raise ValueError(f"M√¥ h√¨nh y√™u c·∫ßu {n_inputs} ƒë·∫ßu v√†o, kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
    elif selected_model_name == "Informer":
        data_seq_x_tensor = tf.convert_to_tensor(data_seq_x, dtype=tf.float32)
        logger.info(f"Shape of data_seq_x_tensor: {data_seq_x_tensor.shape}")
        try:
            signatures = list(model.signatures.keys())
            logger.info(f"Signatures available: {signatures}")
            if "serving_default" in signatures:
                y_pred_scaled = model.signatures["serving_default"](inputs=data_seq_x_tensor)
                y_pred_scaled = y_pred_scaled['output_0']
            else:
                raise ValueError("Kh√¥ng t√¨m th·∫•y signature 'serving_default'")
        except Exception as e:
            logger.error(f"L·ªói khi g·ªçi m√¥ h√¨nh Informer: {e}")
            st.error(f"‚ö†Ô∏è L·ªói g·ªçi m√¥ h√¨nh Informer: {e}")
            st.stop()
        y_pred_scaled = y_pred_scaled.numpy()

    logger.info(f"Shape of y_pred_scaled: {y_pred_scaled.shape}")

    if y_pred_scaled.ndim == 3:
        y_pred_scaled_2d = y_pred_scaled.reshape(-1, y_pred_scaled.shape[-1])
    else:
        y_pred_scaled_2d = y_pred_scaled
    y_pred = scaler_y.inverse_transform(y_pred_scaled_2d)
    y_pred = y_pred.flatten()[:len(time_idx)]

    df_result = pd.DataFrame({"Timestamp": time_idx, "D·ª± b√°o": y_pred}).set_index("Timestamp")
    df_result = df_result[(df_result.index >= start_date) & (df_result.index <= end_date)]
    
    df_result["Ng√†y/Th√°ng"] = df_result.index.strftime("%d/%m")
    df_result = df_result[["Ng√†y/Th√°ng", "D·ª± b√°o"]]

    logger.info(f"‚úÖ D·ª± b√°o th√†nh c√¥ng v·ªõi {len(df_result)} d√≤ng.")
except Exception as e:
    logger.error(f"L·ªói d·ª± b√°o: {e}")
    st.error(f"‚ö†Ô∏è L·ªói d·ª± b√°o: {e}")
    st.stop()

# === Hi·ªÉn th·ªã k·∫øt qu·∫£ ===
st.success(f"‚úÖ ƒê√£ d·ª± b√°o th√†nh c√¥ng v·ªõi m√¥ h√¨nh {selected_model_name}")

col1, col2 = st.columns([2, 1])
with col1:
    st.subheader("üìà Bi·ªÉu ƒë·ªì D·ª± b√°o")
    df_historical = df[[target]].rename(columns={target: "D·ªØ li·ªáu g·ªëc"}) if not is_future_forecast else pd.DataFrame()
    df_result_plot = pd.DataFrame({"D·ª± b√°o": df_result["D·ª± b√°o"]}, index=df_result.index)
    df_combined = pd.concat([df_historical, df_result_plot]) if not df_historical.empty else df_result_plot
    fig, ax = plt.subplots(figsize=(12, 5))
    df_combined.plot(ax=ax, color={"D·ªØ li·ªáu g·ªëc": "green", "D·ª± b√°o": "royalblue"}, label=["D·ªØ li·ªáu g·ªëc", "D·ª± b√°o"])
    ax.set_ylabel("Nhu c·∫ßu ƒëi·ªán (kWh)")
    ax.set_xlabel("Th·ªùi gian")
    ax.set_title(f"D·ª± b√°o nhu c·∫ßu ƒëi·ªán ({start_date.year} - {end_date.year})")
    ax.grid(True, linestyle='--', alpha=0.3)
    ax.legend()
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%d/%m'))
    plt.xticks(rotation=45)
    st.pyplot(fig)

with col2:
    st.subheader("üìä Th·ªëng k√™ d·ª± b√°o")
    st.dataframe(df_result.describe().T, use_container_width=True)
    st.subheader("üßæ To√†n b·ªô d·ªØ li·ªáu d·ª± b√°o")
    st.dataframe(df_result, use_container_width=True)
    csv = df_result.reset_index().to_csv(index=False, encoding="utf-8").encode("utf-8")
    st.download_button("üì• T·∫£i d·ªØ li·ªáu d·ª± b√°o", data=csv, file_name="du_bao_nhu_cau_dien.csv", mime="text/csv")